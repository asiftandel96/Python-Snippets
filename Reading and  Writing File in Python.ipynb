{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE HANDLING IN PYTHON:-\n",
    "\n",
    "One of the most common tasks that you can do with Python is reading and writing files. Whether it’s writing to a simple text file, reading a complicated server log, or even analyzing raw byte data, all of these situations require reading or writing a file.\n",
    "\n",
    "What Is a File?\n",
    "Before we can go into how to work with files in Python, it’s important to understand what exactly a file is and how modern operating systems handle some of their aspects.\n",
    "\n",
    "At its core, a file is a contiguous set of bytes used to store data. This data is organized in a specific format and can be anything as simple as a text file or as complicated as a program executable. In the end, these byte files are then translated into binary 1 and 0 for easier processing by the computer.\n",
    "\n",
    "Files on most modern file systems are composed of three main parts:\n",
    "\n",
    "Header: metadata about the contents of the file (file name, size, type, and so on)\n",
    "\n",
    "Data: contents of the file as written by the creator or editor\n",
    "\n",
    "End of file (EOF): special character that indicates the end of the file\n",
    "\n",
    "\n",
    "What this data represents depends on the format specification used, which is typically represented by an extension. For example, a file that has an extension of .gif most likely conforms to the Graphics Interchange Format specification. There are hundreds, if not thousands, of file extensions out there. For this tutorial, you’ll only deal with .txt or .csv file extensions.\n",
    "\n",
    "\n",
    "File Paths\n",
    "When you access a file on an operating system, a file path is required. The file path is a string that represents the location of a file. It’s broken up into three major parts:\n",
    "\n",
    "1.Folder Path: the file folder location on the file system where subsequent folders are separated by a forward slash / (Unix) or backslash \\ (Windows)\n",
    "\n",
    "2.File Name: the actual name of the file\n",
    "\n",
    "3.Extension: the end of the file path pre-pended with a period (.) used to indicate the file type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A file located within a file structure:-\n",
    "\n",
    "Here’s a quick example. Let’s say you have a file located within a file structure like this:\n",
    "\n",
    "Let’s say you wanted to access the cats.gif file, and your current location was in the same folder as path. In order to access the file, you need to go through the path folder and then the to folder, finally arriving at the cats.gif file. The Folder Path is path/to/. The File Name is cats. The File Extension is .gif. So the full path is path/to/cats.gif.\n",
    "\n",
    "\n",
    "# 2.The file can be simply referenced by the file name and extension \n",
    "\n",
    "Now let’s say that your current location or current working directory (cwd) is in the to folder of our example folder structure. Instead of referring to the cats.gif by the full path of path/to/cats.gif, the file can be simply referenced by the file name and extension cats.gif\n",
    "\n",
    "\n",
    "But what about dog_breeds.txt? How would you access that without using the full path? You can use the special characters double-dot (..) to move one directory up. This means that ../dog_breeds.txt will reference the dog_breeds.txt file from the directory of to:\n",
    "\n",
    "The double-dot (..) can be chained together to traverse multiple directories above the current directory. For example, to access animals.csv from the to folder, you would use ../../animals.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Line Endings\n",
    "\n",
    "\n",
    "One problem often encountered when working with file data is the representation of a new line or line ending. The line ending has its roots from back in the Morse Code era, when a specific pro-sign was used to communicate the end of a transmission or the end of a line.\n",
    "\n",
    "Later, this was standardized for teleprinters by both the International Organization for Standardization (ISO) and the American Standards Association (ASA). ASA standard states that line endings should use the sequence of the Carriage Return (CR or \\r) and the Line Feed (LF or \\n) characters (CR+LF or \\r\\n). The ISO standard however allowed for either the CR+LF characters or just the LF character.\n",
    "\n",
    "Windows uses the CR+LF characters to indicate a new line, while Unix and the newer Mac versions use just the LF character. This can cause some complications when you’re processing files on an operating system that is different than the file’s source.\n",
    "\n",
    "\n",
    "# 2.Character Encodings\n",
    "Another common problem that you may face is the encoding of the byte data. An encoding is a translation from byte data to human readable characters. This is typically done by assigning a numerical value to represent a character. The two most common encodings are the ASCII and UNICODE Formats. ASCII can only store 128 characters, while Unicode can contain up to 1,114,112 characters.\n",
    "\n",
    "ASCII is actually a subset of Unicode (UTF-8), meaning that ASCII and Unicode share the same numerical to character values. It’s important to note that parsing a file with the incorrect character encoding can lead to failures or misrepresentation of the character. For example, if a file was created using the UTF-8 encoding, and you try to parse it using the ASCII encoding, if there is a character that is outside of those 128 values, then an error will be thrown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening and Closing a File in Python\n",
    "\n",
    "When you want to work with a file, the first thing to do is to open it. This is done by invoking the open() built-in function. open() has a single required argument that is the path to the file. open() has a single return, the file object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='Daily Activities.txt' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "file=open('Daily Activities.txt')\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s important to remember that it’s your responsibility to close the file.\n",
    "\n",
    "In most cases, upon termination of an application or script, a file will be closed eventually. However, there is no guarantee when exactly that will happen. This can lead to unwanted behavior including resource leaks. It’s also a best practice within Python (Pythonic) to make sure that your code behaves in a way that is well defined and reduces any unwanted behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.First way of closing the file with the help of try-catch block\n",
    "\n",
    "When you’re manipulating a file, there are two ways that you can use to ensure that a file is closed properly, even when encountering an error. The first way to close a file is to use the try-finally block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow Day:[22 October]\n",
      "\n",
      "1.Python Statement[ 1 hrs]-Done\n",
      "\n",
      "2.Python While Loops[1 hrs]-This need to be done correctly-Done\n",
      "\n",
      "3.Python Functions[2 hrs]-This will take about 2 hrs- Done-Big Topic to Covered\n",
      "\n",
      "4.Python File Handling and OS Module,Python List Comprehension-This Three Topics should be done today\n",
      "\n",
      "[23rd October]\n",
      "\n",
      "1.Python OOP[3 hrs]\n",
      "\n",
      "2.Python Iterators,Decorators,Generators[1 hrs]\n",
      "\n",
      "3.Python Lambda Functions[ 1 hrs]\n",
      "\n",
      "4.Python map and Filter Function[1 hrs]\n",
      "\n",
      "5.Python Exception Handling[1 hrs]\n",
      "\n",
      "[24th October]\n",
      "\n",
      "1.Python DateTime Module[ 2 hrs]\n",
      "\n",
      "2.Python AdvancedOperationOfDict[ 2 hrs]\n",
      "\n",
      "3.Python Regular Expression[2 hrs]\n",
      "\n",
      "[25th October]\n",
      "\n",
      "Python Exercise to boost Coding Techniques\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reader=open('Daily Activities.txt')\n",
    "try:\n",
    "    for f in reader:\n",
    "        print(f)\n",
    "        \n",
    "finally:\n",
    "    reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The second way to close a file is to use the with statement:\n",
    "\n",
    "The with statement automatically takes care of closing the file once it leaves the with block, even in cases of error. I highly recommend that you use the with statement as much as possible, as it allows for cleaner code and makes handling any unexpected errors easier for you.\n",
    "\n",
    "Most likely, you’ll also want to use the second positional argument, mode. This argument is a string that contains multiple characters to represent how you want to open the file. The default and most common is 'r', which represents opening the file in read-only mode as a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow Day:[22 October]\n",
      "1.Python Statement[ 1 hrs]-Done\n",
      "2.Python While Loops[1 hrs]-This need to be done correctly-Done\n",
      "3.Python Functions[2 hrs]-This will take about 2 hrs- Done-Big Topic to Covered\n",
      "4.Python File Handling and OS Module,Python List Comprehension-This Three Topics should be done today\n",
      "[23rd October]\n",
      "1.Python OOP[3 hrs]\n",
      "2.Python Iterators,Decorators,Generators[1 hrs]\n",
      "3.Python Lambda Functions[ 1 hrs]\n",
      "4.Python map and Filter Function[1 hrs]\n",
      "5.Python Exception Handling[1 hrs]\n",
      "[24th October]\n",
      "1.Python DateTime Module[ 2 hrs]\n",
      "2.Python AdvancedOperationOfDict[ 2 hrs]\n",
      "3.Python Regular Expression[2 hrs]\n",
      "[25th October]\n",
      "Python Exercise to boost Coding Techniques\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Daily Activities.txt','r') as reader:\n",
    "    try:\n",
    "            print(reader.read())\n",
    "    finally:\n",
    "        reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character           Meaning\n",
    "\n",
    "'r'                 Open for reading (default)\n",
    "\n",
    "'w'                 Open for writing, truncating (overwriting) the file first\n",
    "\n",
    "'rb' or 'wb'\t    Open in binary mode (read/write using byte data)\n",
    "\n",
    "\n",
    "Let’s go back and talk a little about file objects. A file object is:\n",
    "\n",
    "“an object exposing a file-oriented API (with methods such as read() or write()) to an underlying resource.” (Source)\n",
    "\n",
    "There are three different categories of file objects:\n",
    "\n",
    "Text files\n",
    "\n",
    "Buffered binary files\n",
    "\n",
    "Raw binary files\n",
    "\n",
    "Each of these file types are defined in the io module. Here’s a quick rundown of how everything lines up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Text File Types\n",
    "\n",
    "A text file is the most common file that you’ll encounter. Here are some examples of how these files are opened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n"
     ]
    }
   ],
   "source": [
    "read_file=open('Daily Activities.txt','r')\n",
    "print(type(read_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these types of files, open() will return a TextIOWrapper file object.This is the default file object returned by open()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Buffered binary files\n",
    "\n",
    "A buffered binary file type is used for reading and writing binary files. Here are some examples of how these files are opened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.BufferedReader'>\n",
      "<class '_io.BufferedWriter'>\n"
     ]
    }
   ],
   "source": [
    "binary_file=open('Snip.txt','rb')\n",
    "print(type(binary_file))\n",
    "binary_file=open('Snip.txt','wb')\n",
    "print(type(binary_file))\n",
    "binary_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these types of files, open() will return either a BufferedReader or BufferedWriter file object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Raw binary files\n",
    "\n",
    "A raw file type is:\n",
    "\n",
    "“generally used as a low-level building-block for binary and text streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.FileIO'>\n"
     ]
    }
   ],
   "source": [
    "raw_file=open('5 and 7.png','rb',buffering=0)\n",
    "print(type(raw_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Opened Files\n",
    "\n",
    "Once you’ve opened up a file, you’ll want to read or write to the file. First off, let’s cover reading a file. There are multiple methods that can be called on a file object to help you out:\n",
    "\n",
    "Method for Readng the File:-\n",
    "\n",
    ".read(size=-1)-This reads from the file based on the number of size bytes. If no argument is passed or None or -1 is passed, then the entire file is read.\n",
    "\n",
    ".readline(size=-1)-This reads at most size number of characters from the line. This continues to the end of the line and then wraps back around. If no argument is passed or None or -1 is passed, then the entire line (or rest of the line) is read.\n",
    "\n",
    ".readlines()-This reads the remaining lines from the file object and returns them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.describe()\n",
      "data.info()\n",
      "data.isnull().sum()\n",
      "sns.pairplot(data)\n",
      "rows=2\n",
      "cols=7\n",
      "\n",
      "fig.ax=plt.subplots(nrows=rows,ncols=cols,figsize=(16,4))\n",
      "plt.tight_layout()\n",
      "cols=data.columns\n",
      "index=0\n",
      "\n",
      "for i in range(rows):\n",
      "   for j in range(columns):\n",
      "\tsns.displot(data[col[index],ax[i][j])\n",
      "   index=index+1\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Snippets.txt','r') as reader:\n",
    "        print(reader.read())\n",
    "        reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an example of how to read 5 bytes of a line each time using the Python .readline() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.descr\n",
      "ibe()\n",
      "\n",
      "data.info(\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Snippets.txt','r') as reader:\n",
    "    print(reader.readline(10))\n",
    "    print(reader.readline(10))\n",
    "    print(reader.readline(10))\n",
    "    print(reader.readline(10))\n",
    "    reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an example of how to read the entire file as a list using the Python .readlines() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data.describe()\\n', 'data.info()\\n', 'data.isnull().sum()\\n', 'sns.pairplot(data)\\n', 'rows=2\\n', 'cols=7\\n', '\\n', 'fig.ax=plt.subplots(nrows=rows,ncols=cols,figsize=(16,4))\\n', 'plt.tight_layout()\\n', 'cols=data.columns\\n', 'index=0\\n', '\\n', 'for i in range(rows):\\n', '   for j in range(columns):\\n', '\\tsns.displot(data[col[index],ax[i][j])\\n', '   index=index+1\\n', '\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('Snippets.txt','r') as reader:\n",
    "    print(reader.readlines())\n",
    "    reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example can also be done by using list() to create a list out of the file object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data.describe()\\n', 'data.info()\\n', 'data.isnull().sum()\\n', 'sns.pairplot(data)\\n', 'rows=2\\n', 'cols=7\\n', '\\n', 'fig.ax=plt.subplots(nrows=rows,ncols=cols,figsize=(16,4))\\n', 'plt.tight_layout()\\n', 'cols=data.columns\\n', 'index=0\\n', '\\n', 'for i in range(rows):\\n', '   for j in range(columns):\\n', '\\tsns.displot(data[col[index],ax[i][j])\\n', '   index=index+1\\n', '\\n', '\\n', '\\n']"
     ]
    }
   ],
   "source": [
    "f=open('Snippets.txt','r')\n",
    "print(list(f),end=\"\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating Over Each Line in the File\n",
    "\n",
    "A common thing to do while reading a file is to iterate over each line. Here’s an example of how to use the Python .readline() method to perform that iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the things to be improving in coding:-\n",
      "\n",
      "\n",
      "\n",
      "1) Understanding the Data\n",
      "\n",
      "2) Exploring the Data\n",
      "\n",
      "3) Exploring Data Analysis\n",
      "\n",
      "4)Which model to Use: Using Visualization and Other Techniques\n",
      "\n",
      "5)Improving the Thinking Skills\n",
      "\n",
      "\n",
      "\n",
      "LifeCycle Of Data Science Project:\n",
      "\n",
      "1)Data Collection\n",
      "\n",
      "2)Feature Engineering:Handle Missing Values\n",
      "\n",
      "\n",
      "\n",
      "#Why are there Missing Values?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#What are types of Missing Values:-\n",
      "\n",
      "1)Mean/Median Imputation\n",
      "\n",
      "2)Random Sample Imputation\n",
      "\n",
      "3)Capture NAN values with a new feature\n",
      "\n",
      "4)End of Distribution imputation\n",
      "\n",
      "5)Abrirtary Imputation\n",
      "\n",
      "\n",
      "\n",
      "How to Handle Categorical Values:-\n",
      "\n",
      "1)Frequent Category Imputation\n",
      "\n",
      "2)Adding a variable for NAN Values\n",
      "\n",
      "-Suppose If you we have more frequent categories,we just replace NAN with a new category\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Handling Categorical Features:-\n",
      "\n",
      "1)One Hot Encoding\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Underfitting and Overfitting the Data\n",
      "\n",
      "\n",
      "\n",
      "1)Underfitting Data- Training Accuracy will be lower and Test Accuracy will be also lower\n",
      "\n",
      " High Bias and High Variance\n",
      "\n",
      "\n",
      "\n",
      "2)Overfitting the Data-Training Accuracy will be good but Test Accuracy will be low\n",
      "\n",
      "\n",
      "\n",
      "Low Bias and High Variance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3)the Best Model will be that which has a Low Bias and Low Variance\n",
      "\n",
      "\n",
      "\n",
      "Hyperparameter Tuning\n",
      "\n",
      "\n",
      "\n",
      "Deep Learning:-\n",
      "\n",
      "Neural Network\n",
      "\n",
      "ANN\n",
      "\n",
      "CNN\n",
      "\n",
      "-When the data is in the form of images we generally prefer CNN.\n",
      "\n",
      "-What is Convolution and How Filter are necessary\n",
      "\n",
      "-What is Padding in CNN\n",
      "\n",
      "-Padding is used to save the data present in the images.for e.g Our Input is 6*6 matrix and then we apply a filter with 3*3 matrix and the\n",
      "\n",
      "output come with 4*4 matrix.So ultimately we will use Padding.\n",
      "\n",
      "Two Advantages of Padding:-\n",
      "\n",
      "1)Save the data\n",
      "\n",
      "2)Extract more and more data from the image.\n",
      "\n",
      "Formula:n+2p-f+1\n",
      "\n",
      "-Operation of CNN- Same as ANN operation\n",
      "\n",
      "-Max Pooling Layer of CNN\n",
      "\n",
      "-Data Augmentation of CNN\n",
      "\n",
      "-there is a formula n-f+1 \n",
      "\n",
      "\n",
      "\n",
      "Noted:-\n",
      "\n",
      "What is Transfer Learning\n",
      "\n",
      "RNN-Recurrent Neural Networks\n",
      "\n",
      "RNN Works well with Sequential Data\n",
      "\n",
      "We want to work with time Series Data Recurrent Neural Networks is prefered\n",
      "\n",
      "ReCurrent Neural Network and Its Application:-\n",
      "\n",
      "1)Google Image Search\n",
      "\n",
      "2)Amazon Alexa\n",
      "\n",
      "3)Google Translate-Many to Many RNN\n",
      "\n",
      "Recurrent Neural Network Forward Propogation with Time\n",
      "\n",
      "Recurrent Neural Network BackPropogation \n",
      "\n",
      "Problem in Simple ReCurrent Neural Network:-\n",
      "\n",
      "1)Vanishing Gradient Descent Problem\n",
      "\n",
      "2)Exploding Gradient Descent Problem\n",
      "\n",
      "\n",
      "\n",
      "To Solve this Problem We Will Use LSTM(Long Short Term Memory) RNN\n",
      "\n",
      "\n",
      "\n",
      "LSTM RNN\n",
      "\n",
      "LSTM are very sensitive.So we have to scale the data.We will Use MinMaxScaler\n",
      "\n",
      "1)Memory Cell\n",
      "\n",
      "2)Forget Cell\n",
      "\n",
      "3)Input Cell\n",
      "\n",
      "4)Output Cell\n",
      "\n",
      "\n",
      "\n",
      "Word Embedding Techniques:-(Feature Representation)\n",
      "\n",
      "1)Word2Vec\n",
      "\n",
      "2)Glove\n",
      "\n",
      "\n",
      "\n",
      "Bidirectional RNN\n",
      "\n",
      "1)Most Used in NLP Task\n",
      "\n",
      "\n",
      "\n",
      "The main aim of the Bidirectional RNN is to predict the next based on its bidirectional layers(back layers)\n",
      "\n",
      "\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "Not Used for Speech Recognition\n",
      "\n",
      "Slow as compared to LSTM RNN\n",
      "\n",
      "\n",
      "\n",
      "Sequence to Sequence Layers with Neural Networks(Encoder and Decoder)\n",
      "\n",
      "It works very well with Language Translation\n",
      "\n",
      "Image Captioning\n",
      "\n",
      "\n",
      "\n",
      "Problem with Encoders and Decoders\n",
      "\n",
      "1)Encoder and Decoder usually do not perform with Longer sentences\n",
      "\n",
      "2)Researchers experimented with Lower Bleu Score\n",
      "\n",
      "Human Translator\n",
      "\n",
      "1)Translator word in sentences\n",
      "\n",
      "\n",
      "\n",
      "Bidirectional RNN LSTM\n",
      "\n",
      "\n",
      "\n",
      "To Solve this Problems, We Will Use Attention Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Understanding Attention Model Architecture and Maths Intuition\n",
      "\n",
      "Why the Accuracy is decreasing when sentence length is increasing?\n",
      "\n",
      "1)Not taking output from the LSTM RNN\n",
      "\n",
      "2)We are only taking from the last context vector\n",
      "\n",
      "\n",
      "\n",
      "Sequence to Sequence-Multiple Inputs and Multiple Outputs(Complete Cell Not Neuron)\n",
      "\n",
      "\n",
      "\n",
      "Sequence t Vec-Multiple Inputs and One Vec\n",
      "\n",
      "\n",
      "\n",
      "Vec to Sequence-One Input and Multiple Outputs\n",
      "\n",
      "\n",
      "\n",
      "vec to vec-One Input and One Output \n",
      "\n",
      "\n",
      "\n",
      "Encoder and Decoder Applications:\n",
      "\n",
      "Language Translation\n",
      "\n",
      "Name Entity Recognition\n",
      "\n",
      "Question and Answer Chatbots\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attention Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Deep Learning Concepts Covered:-\n",
      "\n",
      "1)ANN\n",
      "\n",
      "2)CNN\n",
      "\n",
      "3)RNN\n",
      "\n",
      "4)RNN LSTM\n",
      "\n",
      "5)Bidirectional RNN LSTM\n",
      "\n",
      "6)Encoder and Decoder\n",
      "\n",
      "7)Attention Models\n",
      "\n",
      "8)Transformers\n",
      "\n",
      "9)BERT\n",
      "\n",
      "10)AlexNet\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "Variation\n",
      "\n",
      "(n+2p-f)/s+1\n",
      "\n",
      "11)VGGNet\n",
      "\n",
      "Advantages over AlexNet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doubt:\n",
      "\n",
      "Cross-Validation\n",
      "\n",
      "RMSE\n",
      "\n",
      "How to handle multicollinearity- Somewhat Cleared\n",
      "\n",
      "Scaling Problem\n",
      "\n",
      "Improving to Understand\n",
      "\n",
      "I need to improve my coding skills\n",
      "\n",
      "Handling Outlier Detection and Treatment\n",
      "\n",
      "Improving EDA\n",
      "\n",
      "How to do other transformation(i.e boxcox transformation,exponential transformation)\n",
      "\n",
      "Problems with One-Hot Encoding:\n",
      "\n",
      "Adding too many features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Engineering:-\n",
      "\n",
      "How to perform one-hot encoding for multi-categorical variables:\n",
      "\n",
      "e.g If suppose there are more than 100 feature in a categorical variables then it is not suitable to perform one-hot encoding because it \n",
      "\n",
      "will lead to curve of dimensionality\n",
      "\n",
      "\n",
      "\n",
      "So to solve this problem,we can take 10 most frequent features in the categorical variables\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Different Types of Encoding Techniques(Categorical Variable) in Feature Engineering:-\n",
      "\n",
      "\n",
      "\n",
      "Types of Categorical Variables:\n",
      "\n",
      "1)Nominal Categorical Variables e.g gender,state( No ordering of feature)\n",
      "\n",
      "2)Ordinal Categorical Variables- Rearrange the category based on Rank\n",
      "\n",
      "\n",
      "\n",
      "1)Nominal Encoding\n",
      "\n",
      "-One hot Encoding\n",
      "\n",
      "Advantages:\n",
      "\n",
      "Easy to Implement\n",
      "\n",
      "DisAdvantages:\n",
      "\n",
      "If there are more than 50 or above features then it will unnecessarily add columns which lead to curve of dimensionality\n",
      "\n",
      "-One hot Encoding for many categorical variables\n",
      "\n",
      "-Mean Encoding\n",
      "\n",
      "Feature with o/p  variable\n",
      "\n",
      "Assign mean to the value of the rows\n",
      "\n",
      "e.g Pincode\n",
      "\n",
      "\n",
      "\n",
      "2)Ordinal Encoding\n",
      "\n",
      "-Label Encoding\n",
      "\n",
      "\n",
      "\n",
      "-Target Guided Ordinal Encoding\n",
      "\n",
      "-Feature with o/p  variable\n",
      "\n",
      "-Assigning rank based  on mean value\n",
      "\n",
      "\n",
      "\n",
      "Why do you need Feature Scaling:-\n",
      "\n",
      "Algorithms where we need Feature Scaling:-\n",
      "\n",
      "1)Linear Regression-Gradient Descient- for coefficient(bo+b1) \n",
      "\n",
      "2)KNN-Euclidean Distance\n",
      "\n",
      "Features     Height   Weight   BMI\n",
      "\n",
      "-Magnitude   180 cm    86 kg\n",
      "\n",
      "-Units       170 cm    90 kg\n",
      "\n",
      "Algorithm where we do not need Feature Scaling:-\n",
      "\n",
      "1)Decision Trees\n",
      "\n",
      "2)Random Forest\n",
      "\n",
      "3)Ensemble Techniques\n",
      "\n",
      "\n",
      "\n",
      "Handling Missing Values in Categorical Variables:-\n",
      "\n",
      "1)Delete the Rows\n",
      "\n",
      "-Deleting the Rows is not at all efficient if we have small rows\n",
      "\n",
      "2)Replace with the most frequent variables\n",
      "\n",
      "-This process is good if we have balanced dataset so  we can replace the most frequent variable with the mode.\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "this is not good when we have imbalanced dataset\n",
      "\n",
      "3)Apply classifier algorithm to predict(This is the most efficient technique)\n",
      "\n",
      "we can take missing variables features as the output variables and other variable as the independent variables so we can apply classifier \n",
      "\n",
      "algorithm to predict and then all the missing values will be imputed as there result\n",
      "\n",
      "4)Apply unsupervised Machine Learning Algorithm\n",
      "\n",
      " \n",
      "\n",
      "Handle Categorical Feature many Categories(Count/Frequency Encoding)\n",
      "\n",
      "\n",
      "\n",
      "Replacing the label_name with the frequency value\n",
      "\n",
      "\n",
      "\n",
      "Advantages:-\n",
      "\n",
      "1)It is very simple to implement\n",
      "\n",
      "2) Do not increase the feature dimensional space\n",
      "\n",
      "\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "1)If some of the labels have same count,then they will be replaced with the same count and will loose some information \n",
      "\n",
      "2)Add somewhat arbitrary numbers,and therefore weights to different labels,that may not related to  their predictive power\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Handle Categorical Features(Ordinal Encoding):-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Science Projects:Dataset must be collected from multiple data sources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File Handling using While Loop\n",
    "with open('Snip.txt','r') as reader:\n",
    "    line=reader.readline()\n",
    "    while line!='':\n",
    "        print(line)\n",
    "        line=reader.readline() # Counter parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way you could iterate over each line in the file is to use the Python .readlines() method of the file object. Remember, .readlines() returns a list where each element in the list represents a line in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the things to be improving in coding:-\n",
      "\n",
      "\n",
      "\n",
      "1) Understanding the Data\n",
      "\n",
      "2) Exploring the Data\n",
      "\n",
      "3) Exploring Data Analysis\n",
      "\n",
      "4)Which model to Use: Using Visualization and Other Techniques\n",
      "\n",
      "5)Improving the Thinking Skills\n",
      "\n",
      "\n",
      "\n",
      "LifeCycle Of Data Science Project:\n",
      "\n",
      "1)Data Collection\n",
      "\n",
      "2)Feature Engineering:Handle Missing Values\n",
      "\n",
      "\n",
      "\n",
      "#Why are there Missing Values?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#What are types of Missing Values:-\n",
      "\n",
      "1)Mean/Median Imputation\n",
      "\n",
      "2)Random Sample Imputation\n",
      "\n",
      "3)Capture NAN values with a new feature\n",
      "\n",
      "4)End of Distribution imputation\n",
      "\n",
      "5)Abrirtary Imputation\n",
      "\n",
      "\n",
      "\n",
      "How to Handle Categorical Values:-\n",
      "\n",
      "1)Frequent Category Imputation\n",
      "\n",
      "2)Adding a variable for NAN Values\n",
      "\n",
      "-Suppose If you we have more frequent categories,we just replace NAN with a new category\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Handling Categorical Features:-\n",
      "\n",
      "1)One Hot Encoding\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Underfitting and Overfitting the Data\n",
      "\n",
      "\n",
      "\n",
      "1)Underfitting Data- Training Accuracy will be lower and Test Accuracy will be also lower\n",
      "\n",
      " High Bias and High Variance\n",
      "\n",
      "\n",
      "\n",
      "2)Overfitting the Data-Training Accuracy will be good but Test Accuracy will be low\n",
      "\n",
      "\n",
      "\n",
      "Low Bias and High Variance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3)the Best Model will be that which has a Low Bias and Low Variance\n",
      "\n",
      "\n",
      "\n",
      "Hyperparameter Tuning\n",
      "\n",
      "\n",
      "\n",
      "Deep Learning:-\n",
      "\n",
      "Neural Network\n",
      "\n",
      "ANN\n",
      "\n",
      "CNN\n",
      "\n",
      "-When the data is in the form of images we generally prefer CNN.\n",
      "\n",
      "-What is Convolution and How Filter are necessary\n",
      "\n",
      "-What is Padding in CNN\n",
      "\n",
      "-Padding is used to save the data present in the images.for e.g Our Input is 6*6 matrix and then we apply a filter with 3*3 matrix and the\n",
      "\n",
      "output come with 4*4 matrix.So ultimately we will use Padding.\n",
      "\n",
      "Two Advantages of Padding:-\n",
      "\n",
      "1)Save the data\n",
      "\n",
      "2)Extract more and more data from the image.\n",
      "\n",
      "Formula:n+2p-f+1\n",
      "\n",
      "-Operation of CNN- Same as ANN operation\n",
      "\n",
      "-Max Pooling Layer of CNN\n",
      "\n",
      "-Data Augmentation of CNN\n",
      "\n",
      "-there is a formula n-f+1 \n",
      "\n",
      "\n",
      "\n",
      "Noted:-\n",
      "\n",
      "What is Transfer Learning\n",
      "\n",
      "RNN-Recurrent Neural Networks\n",
      "\n",
      "RNN Works well with Sequential Data\n",
      "\n",
      "We want to work with time Series Data Recurrent Neural Networks is prefered\n",
      "\n",
      "ReCurrent Neural Network and Its Application:-\n",
      "\n",
      "1)Google Image Search\n",
      "\n",
      "2)Amazon Alexa\n",
      "\n",
      "3)Google Translate-Many to Many RNN\n",
      "\n",
      "Recurrent Neural Network Forward Propogation with Time\n",
      "\n",
      "Recurrent Neural Network BackPropogation \n",
      "\n",
      "Problem in Simple ReCurrent Neural Network:-\n",
      "\n",
      "1)Vanishing Gradient Descent Problem\n",
      "\n",
      "2)Exploding Gradient Descent Problem\n",
      "\n",
      "\n",
      "\n",
      "To Solve this Problem We Will Use LSTM(Long Short Term Memory) RNN\n",
      "\n",
      "\n",
      "\n",
      "LSTM RNN\n",
      "\n",
      "LSTM are very sensitive.So we have to scale the data.We will Use MinMaxScaler\n",
      "\n",
      "1)Memory Cell\n",
      "\n",
      "2)Forget Cell\n",
      "\n",
      "3)Input Cell\n",
      "\n",
      "4)Output Cell\n",
      "\n",
      "\n",
      "\n",
      "Word Embedding Techniques:-(Feature Representation)\n",
      "\n",
      "1)Word2Vec\n",
      "\n",
      "2)Glove\n",
      "\n",
      "\n",
      "\n",
      "Bidirectional RNN\n",
      "\n",
      "1)Most Used in NLP Task\n",
      "\n",
      "\n",
      "\n",
      "The main aim of the Bidirectional RNN is to predict the next based on its bidirectional layers(back layers)\n",
      "\n",
      "\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "Not Used for Speech Recognition\n",
      "\n",
      "Slow as compared to LSTM RNN\n",
      "\n",
      "\n",
      "\n",
      "Sequence to Sequence Layers with Neural Networks(Encoder and Decoder)\n",
      "\n",
      "It works very well with Language Translation\n",
      "\n",
      "Image Captioning\n",
      "\n",
      "\n",
      "\n",
      "Problem with Encoders and Decoders\n",
      "\n",
      "1)Encoder and Decoder usually do not perform with Longer sentences\n",
      "\n",
      "2)Researchers experimented with Lower Bleu Score\n",
      "\n",
      "Human Translator\n",
      "\n",
      "1)Translator word in sentences\n",
      "\n",
      "\n",
      "\n",
      "Bidirectional RNN LSTM\n",
      "\n",
      "\n",
      "\n",
      "To Solve this Problems, We Will Use Attention Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Understanding Attention Model Architecture and Maths Intuition\n",
      "\n",
      "Why the Accuracy is decreasing when sentence length is increasing?\n",
      "\n",
      "1)Not taking output from the LSTM RNN\n",
      "\n",
      "2)We are only taking from the last context vector\n",
      "\n",
      "\n",
      "\n",
      "Sequence to Sequence-Multiple Inputs and Multiple Outputs(Complete Cell Not Neuron)\n",
      "\n",
      "\n",
      "\n",
      "Sequence t Vec-Multiple Inputs and One Vec\n",
      "\n",
      "\n",
      "\n",
      "Vec to Sequence-One Input and Multiple Outputs\n",
      "\n",
      "\n",
      "\n",
      "vec to vec-One Input and One Output \n",
      "\n",
      "\n",
      "\n",
      "Encoder and Decoder Applications:\n",
      "\n",
      "Language Translation\n",
      "\n",
      "Name Entity Recognition\n",
      "\n",
      "Question and Answer Chatbots\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attention Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Deep Learning Concepts Covered:-\n",
      "\n",
      "1)ANN\n",
      "\n",
      "2)CNN\n",
      "\n",
      "3)RNN\n",
      "\n",
      "4)RNN LSTM\n",
      "\n",
      "5)Bidirectional RNN LSTM\n",
      "\n",
      "6)Encoder and Decoder\n",
      "\n",
      "7)Attention Models\n",
      "\n",
      "8)Transformers\n",
      "\n",
      "9)BERT\n",
      "\n",
      "10)AlexNet\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "Variation\n",
      "\n",
      "(n+2p-f)/s+1\n",
      "\n",
      "11)VGGNet\n",
      "\n",
      "Advantages over AlexNet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doubt:\n",
      "\n",
      "Cross-Validation\n",
      "\n",
      "RMSE\n",
      "\n",
      "How to handle multicollinearity- Somewhat Cleared\n",
      "\n",
      "Scaling Problem\n",
      "\n",
      "Improving to Understand\n",
      "\n",
      "I need to improve my coding skills\n",
      "\n",
      "Handling Outlier Detection and Treatment\n",
      "\n",
      "Improving EDA\n",
      "\n",
      "How to do other transformation(i.e boxcox transformation,exponential transformation)\n",
      "\n",
      "Problems with One-Hot Encoding:\n",
      "\n",
      "Adding too many features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Engineering:-\n",
      "\n",
      "How to perform one-hot encoding for multi-categorical variables:\n",
      "\n",
      "e.g If suppose there are more than 100 feature in a categorical variables then it is not suitable to perform one-hot encoding because it \n",
      "\n",
      "will lead to curve of dimensionality\n",
      "\n",
      "\n",
      "\n",
      "So to solve this problem,we can take 10 most frequent features in the categorical variables\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Different Types of Encoding Techniques(Categorical Variable) in Feature Engineering:-\n",
      "\n",
      "\n",
      "\n",
      "Types of Categorical Variables:\n",
      "\n",
      "1)Nominal Categorical Variables e.g gender,state( No ordering of feature)\n",
      "\n",
      "2)Ordinal Categorical Variables- Rearrange the category based on Rank\n",
      "\n",
      "\n",
      "\n",
      "1)Nominal Encoding\n",
      "\n",
      "-One hot Encoding\n",
      "\n",
      "Advantages:\n",
      "\n",
      "Easy to Implement\n",
      "\n",
      "DisAdvantages:\n",
      "\n",
      "If there are more than 50 or above features then it will unnecessarily add columns which lead to curve of dimensionality\n",
      "\n",
      "-One hot Encoding for many categorical variables\n",
      "\n",
      "-Mean Encoding\n",
      "\n",
      "Feature with o/p  variable\n",
      "\n",
      "Assign mean to the value of the rows\n",
      "\n",
      "e.g Pincode\n",
      "\n",
      "\n",
      "\n",
      "2)Ordinal Encoding\n",
      "\n",
      "-Label Encoding\n",
      "\n",
      "\n",
      "\n",
      "-Target Guided Ordinal Encoding\n",
      "\n",
      "-Feature with o/p  variable\n",
      "\n",
      "-Assigning rank based  on mean value\n",
      "\n",
      "\n",
      "\n",
      "Why do you need Feature Scaling:-\n",
      "\n",
      "Algorithms where we need Feature Scaling:-\n",
      "\n",
      "1)Linear Regression-Gradient Descient- for coefficient(bo+b1) \n",
      "\n",
      "2)KNN-Euclidean Distance\n",
      "\n",
      "Features     Height   Weight   BMI\n",
      "\n",
      "-Magnitude   180 cm    86 kg\n",
      "\n",
      "-Units       170 cm    90 kg\n",
      "\n",
      "Algorithm where we do not need Feature Scaling:-\n",
      "\n",
      "1)Decision Trees\n",
      "\n",
      "2)Random Forest\n",
      "\n",
      "3)Ensemble Techniques\n",
      "\n",
      "\n",
      "\n",
      "Handling Missing Values in Categorical Variables:-\n",
      "\n",
      "1)Delete the Rows\n",
      "\n",
      "-Deleting the Rows is not at all efficient if we have small rows\n",
      "\n",
      "2)Replace with the most frequent variables\n",
      "\n",
      "-This process is good if we have balanced dataset so  we can replace the most frequent variable with the mode.\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "this is not good when we have imbalanced dataset\n",
      "\n",
      "3)Apply classifier algorithm to predict(This is the most efficient technique)\n",
      "\n",
      "we can take missing variables features as the output variables and other variable as the independent variables so we can apply classifier \n",
      "\n",
      "algorithm to predict and then all the missing values will be imputed as there result\n",
      "\n",
      "4)Apply unsupervised Machine Learning Algorithm\n",
      "\n",
      " \n",
      "\n",
      "Handle Categorical Feature many Categories(Count/Frequency Encoding)\n",
      "\n",
      "\n",
      "\n",
      "Replacing the label_name with the frequency value\n",
      "\n",
      "\n",
      "\n",
      "Advantages:-\n",
      "\n",
      "1)It is very simple to implement\n",
      "\n",
      "2) Do not increase the feature dimensional space\n",
      "\n",
      "\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "1)If some of the labels have same count,then they will be replaced with the same count and will loose some information \n",
      "\n",
      "2)Add somewhat arbitrary numbers,and therefore weights to different labels,that may not related to  their predictive power\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Handle Categorical Features(Ordinal Encoding):-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Science Projects:Dataset must be collected from multiple data sources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File Handling using For Loop\n",
    "with open('Snip.txt','r') as reader:\n",
    "    for line in reader.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above examples can be further simplified by iterating over the file object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the things to be improving in coding:-\n",
      "\n",
      "\n",
      "\n",
      "1) Understanding the Data\n",
      "\n",
      "2) Exploring the Data\n",
      "\n",
      "3) Exploring Data Analysis\n",
      "\n",
      "4)Which model to Use: Using Visualization and Other Techniques\n",
      "\n",
      "5)Improving the Thinking Skills\n",
      "\n",
      "\n",
      "\n",
      "LifeCycle Of Data Science Project:\n",
      "\n",
      "1)Data Collection\n",
      "\n",
      "2)Feature Engineering:Handle Missing Values\n",
      "\n",
      "\n",
      "\n",
      "#Why are there Missing Values?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#What are types of Missing Values:-\n",
      "\n",
      "1)Mean/Median Imputation\n",
      "\n",
      "2)Random Sample Imputation\n",
      "\n",
      "3)Capture NAN values with a new feature\n",
      "\n",
      "4)End of Distribution imputation\n",
      "\n",
      "5)Abrirtary Imputation\n",
      "\n",
      "\n",
      "\n",
      "How to Handle Categorical Values:-\n",
      "\n",
      "1)Frequent Category Imputation\n",
      "\n",
      "2)Adding a variable for NAN Values\n",
      "\n",
      "-Suppose If you we have more frequent categories,we just replace NAN with a new category\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Handling Categorical Features:-\n",
      "\n",
      "1)One Hot Encoding\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Underfitting and Overfitting the Data\n",
      "\n",
      "\n",
      "\n",
      "1)Underfitting Data- Training Accuracy will be lower and Test Accuracy will be also lower\n",
      "\n",
      " High Bias and High Variance\n",
      "\n",
      "\n",
      "\n",
      "2)Overfitting the Data-Training Accuracy will be good but Test Accuracy will be low\n",
      "\n",
      "\n",
      "\n",
      "Low Bias and High Variance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3)the Best Model will be that which has a Low Bias and Low Variance\n",
      "\n",
      "\n",
      "\n",
      "Hyperparameter Tuning\n",
      "\n",
      "\n",
      "\n",
      "Deep Learning:-\n",
      "\n",
      "Neural Network\n",
      "\n",
      "ANN\n",
      "\n",
      "CNN\n",
      "\n",
      "-When the data is in the form of images we generally prefer CNN.\n",
      "\n",
      "-What is Convolution and How Filter are necessary\n",
      "\n",
      "-What is Padding in CNN\n",
      "\n",
      "-Padding is used to save the data present in the images.for e.g Our Input is 6*6 matrix and then we apply a filter with 3*3 matrix and the\n",
      "\n",
      "output come with 4*4 matrix.So ultimately we will use Padding.\n",
      "\n",
      "Two Advantages of Padding:-\n",
      "\n",
      "1)Save the data\n",
      "\n",
      "2)Extract more and more data from the image.\n",
      "\n",
      "Formula:n+2p-f+1\n",
      "\n",
      "-Operation of CNN- Same as ANN operation\n",
      "\n",
      "-Max Pooling Layer of CNN\n",
      "\n",
      "-Data Augmentation of CNN\n",
      "\n",
      "-there is a formula n-f+1 \n",
      "\n",
      "\n",
      "\n",
      "Noted:-\n",
      "\n",
      "What is Transfer Learning\n",
      "\n",
      "RNN-Recurrent Neural Networks\n",
      "\n",
      "RNN Works well with Sequential Data\n",
      "\n",
      "We want to work with time Series Data Recurrent Neural Networks is prefered\n",
      "\n",
      "ReCurrent Neural Network and Its Application:-\n",
      "\n",
      "1)Google Image Search\n",
      "\n",
      "2)Amazon Alexa\n",
      "\n",
      "3)Google Translate-Many to Many RNN\n",
      "\n",
      "Recurrent Neural Network Forward Propogation with Time\n",
      "\n",
      "Recurrent Neural Network BackPropogation \n",
      "\n",
      "Problem in Simple ReCurrent Neural Network:-\n",
      "\n",
      "1)Vanishing Gradient Descent Problem\n",
      "\n",
      "2)Exploding Gradient Descent Problem\n",
      "\n",
      "\n",
      "\n",
      "To Solve this Problem We Will Use LSTM(Long Short Term Memory) RNN\n",
      "\n",
      "\n",
      "\n",
      "LSTM RNN\n",
      "\n",
      "LSTM are very sensitive.So we have to scale the data.We will Use MinMaxScaler\n",
      "\n",
      "1)Memory Cell\n",
      "\n",
      "2)Forget Cell\n",
      "\n",
      "3)Input Cell\n",
      "\n",
      "4)Output Cell\n",
      "\n",
      "\n",
      "\n",
      "Word Embedding Techniques:-(Feature Representation)\n",
      "\n",
      "1)Word2Vec\n",
      "\n",
      "2)Glove\n",
      "\n",
      "\n",
      "\n",
      "Bidirectional RNN\n",
      "\n",
      "1)Most Used in NLP Task\n",
      "\n",
      "\n",
      "\n",
      "The main aim of the Bidirectional RNN is to predict the next based on its bidirectional layers(back layers)\n",
      "\n",
      "\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "Not Used for Speech Recognition\n",
      "\n",
      "Slow as compared to LSTM RNN\n",
      "\n",
      "\n",
      "\n",
      "Sequence to Sequence Layers with Neural Networks(Encoder and Decoder)\n",
      "\n",
      "It works very well with Language Translation\n",
      "\n",
      "Image Captioning\n",
      "\n",
      "\n",
      "\n",
      "Problem with Encoders and Decoders\n",
      "\n",
      "1)Encoder and Decoder usually do not perform with Longer sentences\n",
      "\n",
      "2)Researchers experimented with Lower Bleu Score\n",
      "\n",
      "Human Translator\n",
      "\n",
      "1)Translator word in sentences\n",
      "\n",
      "\n",
      "\n",
      "Bidirectional RNN LSTM\n",
      "\n",
      "\n",
      "\n",
      "To Solve this Problems, We Will Use Attention Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Understanding Attention Model Architecture and Maths Intuition\n",
      "\n",
      "Why the Accuracy is decreasing when sentence length is increasing?\n",
      "\n",
      "1)Not taking output from the LSTM RNN\n",
      "\n",
      "2)We are only taking from the last context vector\n",
      "\n",
      "\n",
      "\n",
      "Sequence to Sequence-Multiple Inputs and Multiple Outputs(Complete Cell Not Neuron)\n",
      "\n",
      "\n",
      "\n",
      "Sequence t Vec-Multiple Inputs and One Vec\n",
      "\n",
      "\n",
      "\n",
      "Vec to Sequence-One Input and Multiple Outputs\n",
      "\n",
      "\n",
      "\n",
      "vec to vec-One Input and One Output \n",
      "\n",
      "\n",
      "\n",
      "Encoder and Decoder Applications:\n",
      "\n",
      "Language Translation\n",
      "\n",
      "Name Entity Recognition\n",
      "\n",
      "Question and Answer Chatbots\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attention Model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Deep Learning Concepts Covered:-\n",
      "\n",
      "1)ANN\n",
      "\n",
      "2)CNN\n",
      "\n",
      "3)RNN\n",
      "\n",
      "4)RNN LSTM\n",
      "\n",
      "5)Bidirectional RNN LSTM\n",
      "\n",
      "6)Encoder and Decoder\n",
      "\n",
      "7)Attention Models\n",
      "\n",
      "8)Transformers\n",
      "\n",
      "9)BERT\n",
      "\n",
      "10)AlexNet\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "Variation\n",
      "\n",
      "(n+2p-f)/s+1\n",
      "\n",
      "11)VGGNet\n",
      "\n",
      "Advantages over AlexNet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doubt:\n",
      "\n",
      "Cross-Validation\n",
      "\n",
      "RMSE\n",
      "\n",
      "How to handle multicollinearity- Somewhat Cleared\n",
      "\n",
      "Scaling Problem\n",
      "\n",
      "Improving to Understand\n",
      "\n",
      "I need to improve my coding skills\n",
      "\n",
      "Handling Outlier Detection and Treatment\n",
      "\n",
      "Improving EDA\n",
      "\n",
      "How to do other transformation(i.e boxcox transformation,exponential transformation)\n",
      "\n",
      "Problems with One-Hot Encoding:\n",
      "\n",
      "Adding too many features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Engineering:-\n",
      "\n",
      "How to perform one-hot encoding for multi-categorical variables:\n",
      "\n",
      "e.g If suppose there are more than 100 feature in a categorical variables then it is not suitable to perform one-hot encoding because it \n",
      "\n",
      "will lead to curve of dimensionality\n",
      "\n",
      "\n",
      "\n",
      "So to solve this problem,we can take 10 most frequent features in the categorical variables\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Different Types of Encoding Techniques(Categorical Variable) in Feature Engineering:-\n",
      "\n",
      "\n",
      "\n",
      "Types of Categorical Variables:\n",
      "\n",
      "1)Nominal Categorical Variables e.g gender,state( No ordering of feature)\n",
      "\n",
      "2)Ordinal Categorical Variables- Rearrange the category based on Rank\n",
      "\n",
      "\n",
      "\n",
      "1)Nominal Encoding\n",
      "\n",
      "-One hot Encoding\n",
      "\n",
      "Advantages:\n",
      "\n",
      "Easy to Implement\n",
      "\n",
      "DisAdvantages:\n",
      "\n",
      "If there are more than 50 or above features then it will unnecessarily add columns which lead to curve of dimensionality\n",
      "\n",
      "-One hot Encoding for many categorical variables\n",
      "\n",
      "-Mean Encoding\n",
      "\n",
      "Feature with o/p  variable\n",
      "\n",
      "Assign mean to the value of the rows\n",
      "\n",
      "e.g Pincode\n",
      "\n",
      "\n",
      "\n",
      "2)Ordinal Encoding\n",
      "\n",
      "-Label Encoding\n",
      "\n",
      "\n",
      "\n",
      "-Target Guided Ordinal Encoding\n",
      "\n",
      "-Feature with o/p  variable\n",
      "\n",
      "-Assigning rank based  on mean value\n",
      "\n",
      "\n",
      "\n",
      "Why do you need Feature Scaling:-\n",
      "\n",
      "Algorithms where we need Feature Scaling:-\n",
      "\n",
      "1)Linear Regression-Gradient Descient- for coefficient(bo+b1) \n",
      "\n",
      "2)KNN-Euclidean Distance\n",
      "\n",
      "Features     Height   Weight   BMI\n",
      "\n",
      "-Magnitude   180 cm    86 kg\n",
      "\n",
      "-Units       170 cm    90 kg\n",
      "\n",
      "Algorithm where we do not need Feature Scaling:-\n",
      "\n",
      "1)Decision Trees\n",
      "\n",
      "2)Random Forest\n",
      "\n",
      "3)Ensemble Techniques\n",
      "\n",
      "\n",
      "\n",
      "Handling Missing Values in Categorical Variables:-\n",
      "\n",
      "1)Delete the Rows\n",
      "\n",
      "-Deleting the Rows is not at all efficient if we have small rows\n",
      "\n",
      "2)Replace with the most frequent variables\n",
      "\n",
      "-This process is good if we have balanced dataset so  we can replace the most frequent variable with the mode.\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "this is not good when we have imbalanced dataset\n",
      "\n",
      "3)Apply classifier algorithm to predict(This is the most efficient technique)\n",
      "\n",
      "we can take missing variables features as the output variables and other variable as the independent variables so we can apply classifier \n",
      "\n",
      "algorithm to predict and then all the missing values will be imputed as there result\n",
      "\n",
      "4)Apply unsupervised Machine Learning Algorithm\n",
      "\n",
      " \n",
      "\n",
      "Handle Categorical Feature many Categories(Count/Frequency Encoding)\n",
      "\n",
      "\n",
      "\n",
      "Replacing the label_name with the frequency value\n",
      "\n",
      "\n",
      "\n",
      "Advantages:-\n",
      "\n",
      "1)It is very simple to implement\n",
      "\n",
      "2) Do not increase the feature dimensional space\n",
      "\n",
      "\n",
      "\n",
      "DisAdvantages:-\n",
      "\n",
      "1)If some of the labels have same count,then they will be replaced with the same count and will loose some information \n",
      "\n",
      "2)Add somewhat arbitrary numbers,and therefore weights to different labels,that may not related to  their predictive power\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Handle Categorical Features(Ordinal Encoding):-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Science Projects:Dataset must be collected from multiple data sources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Snip.txt','r') as reader:\n",
    "    for line in reader:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Files In Python\n",
    "\n",
    "Now let’s dive into writing files. As with reading files, file objects have multiple methods that are useful for writing to a file:\n",
    "\n",
    "\n",
    "\n",
    "Method:-\n",
    "\n",
    "write(string)            This writes the string to the file.\n",
    "\n",
    "writelines(seq)\t         This writes the sequence to the file. No line endings are appended to each sequence item. It’s up to                             you to add the appropriate line ending(s).\n",
    "\n",
    "\n",
    "Here’s a quick example of using .write() and .writelines():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Files In Python\n",
    "with open('Andrew NG Notes.txt','r') as reader:\n",
    "    read_file=reader.readlines()\n",
    "    with open('Notes.txt','w') as writer:\n",
    "        writer.writelines(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write File in Reversed Order In Python\n",
    "with open('Andrew NG Notes.txt','r') as reader:\n",
    "    read_file=reader.readlines()\n",
    "    with open('Notes_reversed.txt','w') as writer:\n",
    "        writer.writelines(reversed(read_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Bytes \n",
    "\n",
    "Sometimes, you may need to work with files using byte strings. This is done by adding the 'b' character to the mode argument. All of the same methods for the file object apply. However, each of the methods expect and return a bytes object instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Internet Company:\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "with open('Notes.txt','rb') as reader:\n",
    "    print(reader.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually open that file in Python and examine the contents! Since the .png file format is well defined, the header of the file is 8 bytes broken up like this:\n",
    "\n",
    "Value\t        Interpretation\n",
    "\n",
    "0x89\t        A “magic” number to indicate that this is the start of a PNG\n",
    "\n",
    "0x50 0x4E 0x47\tPNG in ASCII\n",
    "\n",
    "0x0D 0x0A\t    A DOS style line ending \\r\\n\n",
    "\n",
    "0x1A\t        A DOS style EOF character\n",
    "\n",
    "0x0A\t        A Unix style line ending \\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x89'\n",
      "b'PN'\n",
      "b'G\\r\\n'\n",
      "b'\\x1a\\n\\x00\\x00'\n",
      "b'\\x00\\rIHD'\n",
      "b'R\\x00\\x00\\x05V\\x00'\n",
      "b'\\x00\\x03\\x00\\x08\\x06\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('2019-12-14 (2).png','rb') as byte_reader:\n",
    "    print(byte_reader.read(1)) # This is the starting of the PNG File(\\x89 as shown in the output)\n",
    "    print(byte_reader.read(2)) \n",
    "    print(byte_reader.read(3)) # DOS Style line ending\n",
    "    print(byte_reader.read(4)) # Unix Style line ending\n",
    "    print(byte_reader.read(5)) \n",
    "    print(byte_reader.read(6))\n",
    "    print(byte_reader.read(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __file__\n",
    "\n",
    "\n",
    "The __file__ attribute is a special attribute of modules, similar to __name__. It is:\n",
    "\n",
    "“the pathname of the file from which the module was loaded, if it was loaded from a file\n",
    "\n",
    "\n",
    "To re-iterate, __file__ returns the path relative to where the initial Python script was called. If you need the full system path, you can use os.getcwd() to get the current working directory of your executing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Python Basics\n",
      "c:\\users\\asif\\appdata\\local\\programs\\python\\python38\\lib\\os.py\n",
      "os\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "\n",
    "print(os.getcwd()) # getting the current directory of the file\n",
    "print(os.__file__)\n",
    "print(os.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Tree Structure in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                       1.First Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Basics/\n",
      "    2019-12-14 (2).png\n",
      "    5 and 7.png\n",
      "    Andrew NG Notes.txt\n",
      "    Daily Activities.txt\n",
      "    debug.log\n",
      "    Notes.txt\n",
      "    Notes_reversed.txt\n",
      "    Python Basics.ipynb\n",
      "    Reading and  Writing File in Python.ipynb\n",
      "    Session 1.1 - Python-Installation and Basics.ipynb\n",
      "    Snip.txt\n",
      "    Snippets.txt\n",
      "    .ipynb_checkpoints/\n",
      "        Python Basics-checkpoint\n",
      "        Python Basics-checkpoint.ipynb\n",
      "        Reading and  Writing File in Python-checkpoint.ipynb\n",
      "        Session 1.1 - Python-Installation and Basics-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_files(\"E:\\\\Python Basics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files(\"E:\\\\Pen Drive Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DisplayablePath(object):\n",
    "    display_filename_prefix_middle = '├──'\n",
    "    display_filename_prefix_last = '└──'\n",
    "    display_parent_prefix_middle = '    '\n",
    "    display_parent_prefix_last = '│   '\n",
    "\n",
    "    def __init__(self, path, parent_path, is_last):\n",
    "        self.path = Path(str(path))\n",
    "        self.parent = parent_path\n",
    "        self.is_last = is_last\n",
    "        if self.parent:\n",
    "            self.depth = self.parent.depth + 1\n",
    "        else:\n",
    "            self.depth = 0\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    @classmethod\n",
    "    def make_tree(cls, root, parent=None, is_last=False, criteria=None):\n",
    "        root = Path(str(root))\n",
    "        criteria = criteria or cls._default_criteria\n",
    "\n",
    "        displayable_root = cls(root, parent, is_last)\n",
    "        yield displayable_root\n",
    "\n",
    "        children = sorted(list(path\n",
    "                               for path in root.iterdir()\n",
    "                               if criteria(path)),\n",
    "                          key=lambda s: str(s).lower())\n",
    "        count = 1\n",
    "        for path in children:\n",
    "            is_last = count == len(children)\n",
    "            if path.is_dir():\n",
    "                yield from cls.make_tree(path,\n",
    "                                         parent=displayable_root,\n",
    "                                         is_last=is_last,\n",
    "                                         criteria=criteria)\n",
    "            else:\n",
    "                yield cls(path, displayable_root, is_last)\n",
    "            count += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _default_criteria(cls, path):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    def displayable(self):\n",
    "        if self.parent is None:\n",
    "            return self.displayname\n",
    "\n",
    "        _filename_prefix = (self.display_filename_prefix_last\n",
    "                            if self.is_last\n",
    "                            else self.display_filename_prefix_middle)\n",
    "\n",
    "        parts = ['{!s} {!s}'.format(_filename_prefix,\n",
    "                                    self.displayname)]\n",
    "\n",
    "        parent = self.parent\n",
    "        while parent and parent.parent is not None:\n",
    "            parts.append(self.display_parent_prefix_middle\n",
    "                         if parent.is_last\n",
    "                         else self.display_parent_prefix_last)\n",
    "            parent = parent.parent\n",
    "\n",
    "        return ''.join(reversed(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Basics/\n",
      "├── .ipynb_checkpoints/\n",
      "│   ├── Python Basics-checkpoint\n",
      "│   ├── Python Basics-checkpoint.ipynb\n",
      "│   ├── Reading and  Writing File in Python-checkpoint.ipynb\n",
      "│   └── Session 1.1 - Python-Installation and Basics-checkpoint.ipynb\n",
      "├── 2019-12-14 (2).png\n",
      "├── 5 and 7.png\n",
      "├── Andrew NG Notes.txt\n",
      "├── Daily Activities.txt\n",
      "├── debug.log\n",
      "├── Notes.txt\n",
      "├── Notes_reversed.txt\n",
      "├── Python Basics.ipynb\n",
      "├── Reading and  Writing File in Python.ipynb\n",
      "├── Session 1.1 - Python-Installation and Basics.ipynb\n",
      "├── Snip.txt\n",
      "└── Snippets.txt\n"
     ]
    }
   ],
   "source": [
    "paths = DisplayablePath.make_tree(Path('E:\\\\Python Basics'))\n",
    "for path in paths:\n",
    "    print(path.displayable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Python Basics\n",
      "2019-12-14 (2).png\n",
      "5 and 7.png\n",
      "Andrew NG Notes.txt\n",
      "Daily Activities.txt\n",
      "debug.log\n",
      "Notes.txt\n",
      "Notes_reversed.txt\n",
      "Python Basics.ipynb\n",
      "Reading and  Writing File in Python.ipynb\n",
      "Session 1.1 - Python-Installation and Basics.ipynb\n",
      "Snip.txt\n",
      "Snippets.txt\n",
      "E:\\Python Basics\\.ipynb_checkpoints\n",
      "Python Basics-checkpoint\n",
      "Python Basics-checkpoint.ipynb\n",
      "Reading and  Writing File in Python-checkpoint.ipynb\n",
      "Session 1.1 - Python-Installation and Basics-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "for path,dirs,files in os.walk('E:\\\\Python Basics'):\n",
    "    print(path)\n",
    "    for f in files:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending to A File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you may want to append to a file or start writing at the end of an already populated file. This is easily done by using the 'a' character for the mode argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Andrew NG Notes.txt','a') as a_writer:\n",
    "     a_writer.write('\\n Talent Acquistion 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you examine dog_breeds.txt again, you’ll see that the beginning of the file is unchanged and Beagle is now added to the end of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet Company:\n",
      "A/B Testing\n",
      "Shipping faster\n",
      "Decision making by the enginner\n",
      "\n",
      "great AI Company\n",
      "\n",
      "Strategic data Acquistions\n",
      "Unified Data Warehouses\n",
      " Talent Acquistion\n",
      " Talent Acquistion 2\n"
     ]
    }
   ],
   "source": [
    "with open('Andrew NG Notes.txt','r') as reader:\n",
    "    print(reader.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Two Files at the Same Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when you may want to read a file and write to another file at the same time. If you use the example that was shown when you were learning how to write to a file, it can actually be combined into the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file='Andrew NG Notes.txt'\n",
    "second_file='Andrew NG Notes_reversed.txt'\n",
    "with open(first_file,'r') as reader,open(second_file,'w') as writer:\n",
    "    note_file=reader.readlines()\n",
    "    writer.writelines(reversed(note_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_source='Snip.txt'\n",
    "second_source='Snip_r.txt'\n",
    "with open(first_source,'r') as reader,open(second_source,'w') as writer:\n",
    "    snip_file=reader.readlines()\n",
    "    writer.writelines(reversed(snip_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remaining:- Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                 READING AND WRITING CSV FILES IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Is a CSV File?\n",
    "\n",
    "A CSV file (Comma Separated Values file) is a type of plain text file that uses specific structuring to arrange tabular data. Because it’s a plain text file, it can contain only actual text data—in other words, printable ASCII or Unicode characters.\n",
    "\n",
    "The structure of a CSV file is given away by its name. Normally, CSV files use a comma to separate each specific data value.\n",
    "\n",
    "#Examples of CSV Data\n",
    "\n",
    "column 1 name,column 2 name, column 3 name\n",
    "\n",
    "first row data 1,first row data 2,first row data 3\n",
    "\n",
    "second row data 1,second row data 2,second row data 3\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Notice how each piece of data is separated by a comma. Normally, the first line identifies each piece of data—in other words, the name of a data column. Every subsequent line after that is actual data and is limited only by file size constraints.\n",
    "\n",
    "In general, the separator character is called a delimiter, and the comma is not the only one used. Other popular delimiters include the tab (\\t), colon (:) and semi-colon (;) characters. Properly parsing a CSV file requires us to know which delimiter is being used.\n",
    "\n",
    "Where Do CSV Files Come From?\n",
    "\n",
    "CSV files are normally created by programs that handle large amounts of data. They are a convenient way to export data from spreadsheets and databases as well as import or use it in other programs. For example, you might export the results of a data mining program to a CSV file and then import that into a spreadsheet to analyze the data, generate graphs for a presentation, or prepare a report for publication.\n",
    "\n",
    "CSV files are very easy to work with programmatically. Any language that supports text file input and string manipulation (like Python) can work with CSV files directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing CSV Files With Python’s Built-in CSV Library\n",
    "\n",
    "The csv library provides functionality to both read from and write to CSV files. Designed to work out of the box with Excel-generated CSV files, it is easily adapted to work with a variety of CSV formats. The csv library contains objects and other code to read, write, and process data from and to CSV files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV Files With csv\n",
    "\n",
    "Reading from a CSV file is done using the reader object. The CSV file is opened as a text file with Python’s built-in open() function, which returns a file object. This is then passed to the reader, which does the heavy lifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name are name,department,birthday month\n",
      "\tJohn Smith works in the Accounting department, and was born in November.\n",
      "Processed 2 lines.\n",
      "\tErica Meyers works in the IT department, and was born in March.\n",
      "Processed 3 lines.\n",
      "\tAdil Hussain works in the IT department, and was born in April.\n",
      "Processed 4 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Employee.txt','r') as csv_file:\n",
    "    csv_reader=csv.reader(csv_file,delimiter=\",\")\n",
    "    line_count=0\n",
    "    for rows in csv_reader:\n",
    "        if line_count==0:\n",
    "            print(f'Column name are {\",\".join(rows)}')\n",
    "            line_count+=1\n",
    "        else:\n",
    "            print(f'\\t{rows[0]} works in the {rows[1]} department, and was born in {rows[2]}.')\n",
    "            line_count += 1\n",
    "            print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row returned by the reader is a list of String elements containing the data found by removing the delimiters. The first row returned contains the column names, which is handled in a special way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV Files Into a Dictionary With csv\n",
    "\n",
    "Rather than deal with a list of individual String elements, you can read CSV data directly into a dictionary (technically, an Ordered Dictionary) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name are name,department,birthday month\n",
      "\tJohn Smith works in the Accounting department, and was born in November.\n",
      "\tErica Meyers works in the IT department, and was born in March.\n",
      "\tAdil Hussain works in the IT department, and was born in April.\n",
      "Processed 4 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Employee.txt','r') as csv_file:\n",
    "    csv_reader=csv.DictReader(csv_file,delimiter=\",\")\n",
    "    line_count=0\n",
    "    for rows in csv_reader:\n",
    "        if line_count==0:\n",
    "            print(f'Column name are {\",\".join(rows)}')\n",
    "            line_count+=1\n",
    "        print(f'\\t{rows[\"name\"]} works in the {rows[\"department\"]} department, and was born in {rows[\"birthday month\"]}.')\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where did the dictionary keys come from? The first line of the CSV file is assumed to contain the keys to use to build the dictionary. If you don’t have these in your CSV file, you should specify your own keys by setting the fieldnames optional parameter to a list containing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'month', 'market', 'acme']\n",
      "['1', '1/86', '-0.061134', '0.03016']\n",
      "['2', '2/86', '0.00822', '-0.165457']\n",
      "['3', '3/86', '-0.007381', '0.080137']\n",
      "['4', '4/86', '-0.067561', '-0.109917']\n",
      "['5', '5/86', '-0.006238', '-0.114853']\n",
      "['6', '6/86', '-0.044251', '-0.099254']\n",
      "['7', '7/86', '-0.11207', '-0.226846']\n",
      "['8', '8/86', '0.030226', '0.073445']\n",
      "['9', '9/86', '-0.129556', '-0.143064']\n",
      "['10', '10/86', '0.001319', '0.034776']\n",
      "['11', '11/86', '-0.033679', '-0.063375']\n",
      "['12', '12/86', '-0.072795', '-0.058735']\n",
      "['13', '1/87', '0.073396', '0.050214']\n",
      "['14', '2/87', '-0.011618', '0.111165']\n",
      "['15', '3/87', '-0.026852', '-0.127492']\n",
      "['16', '4/87', '-0.040356', '0.054522']\n",
      "['17', '5/87', '-0.047539', '-0.072918']\n",
      "['18', '6/87', '-0.001732', '-0.058979']\n",
      "['19', '7/87', '-0.008899', '0.236147']\n",
      "['20', '8/87', '-0.020837', '-0.094778']\n",
      "['21', '9/87', '-0.084811', '-0.135669']\n",
      "['22', '10/87', '-0.262077', '-0.284796']\n",
      "['23', '11/87', '-0.110167', '-0.171494']\n",
      "['24', '12/87', '0.034955', '0.242616']\n",
      "['25', '1/88', '0.012688', '-0.063518']\n",
      "['26', '2/88', '-0.00217', '-0.117677']\n",
      "['27', '3/88', '-0.073462', '0.201674']\n",
      "['28', '4/88', '-0.043419', '-0.147728']\n",
      "['29', '5/88', '-0.05473', '-0.170885']\n",
      "['30', '6/88', '-0.011755', '-0.014893']\n",
      "['31', '7/88', '-0.061718', '-0.110515']\n",
      "['32', '8/88', '-0.10171', '-0.168769']\n",
      "['33', '9/88', '-0.032705', '-0.135585']\n",
      "['34', '10/88', '-0.045334', '-0.084077']\n",
      "['35', '11/88', '-0.079288', '-0.16455']\n",
      "['36', '12/88', '-0.036233', '0.150269']\n",
      "['37', '1/89', '-0.011494', '-0.015672']\n",
      "['38', '2/89', '-0.093729', '-0.03786']\n",
      "['39', '3/89', '-0.065215', '-0.074712']\n",
      "['40', '4/89', '-0.037113', '-0.10853']\n",
      "['41', '5/89', '-0.044399', '-0.036769']\n",
      "['42', '6/89', '-0.084412', '0.023912']\n",
      "['43', '7/89', '0.003444', '-0.07843']\n",
      "['44', '8/89', '-0.05676', '-0.132199']\n",
      "['45', '9/89', '-0.07897', '-0.110141']\n",
      "['46', '10/89', '-0.105367', '-0.126302']\n",
      "['47', '11/89', '-0.038634', '-0.09573']\n",
      "['48', '12/89', '-0.043261', '0.06574']\n",
      "['49', '1/90', '-0.139773', '-0.120056']\n",
      "['50', '2/90', '-0.059094', '-0.085205']\n",
      "['51', '3/90', '-0.057736', '-0.130433']\n",
      "['52', '4/90', '-0.102524', '-0.116728']\n",
      "['53', '5/90', '0.023881', '-0.078039']\n",
      "['54', '6/90', '-0.079116', '-0.170322']\n",
      "['55', '7/90', '-0.078965', '-0.077727']\n",
      "['56', '8/90', '-0.161359', '-0.277035']\n",
      "['57', '9/90', '-0.119376', '-0.207595']\n",
      "['58', '10/90', '-0.076008', '-0.070515']\n",
      "['59', '11/90', '-0.006444', '-0.046274']\n",
      "['60', '12/90', '-0.026401', '-0.190834']\n"
     ]
    }
   ],
   "source": [
    "# Normally csv file reading\n",
    "\n",
    "with open('acme.csv','r')  as csv_file:\n",
    "    csv_reader=csv.reader(csv_file,delimiter=\",\")\n",
    "    for row in csv_reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.061134\n",
      "0.00822\n",
      "-0.007381\n",
      "-0.067561\n",
      "-0.006238\n",
      "-0.044251\n",
      "-0.11207\n",
      "0.030226\n",
      "-0.129556\n",
      "0.001319\n",
      "-0.033679\n",
      "-0.072795\n",
      "0.073396\n",
      "-0.011618\n",
      "-0.026852\n",
      "-0.040356\n",
      "-0.047539\n",
      "-0.001732\n",
      "-0.008899\n",
      "-0.020837\n",
      "-0.084811\n",
      "-0.262077\n",
      "-0.110167\n",
      "0.034955\n",
      "0.012688\n",
      "-0.00217\n",
      "-0.073462\n",
      "-0.043419\n",
      "-0.05473\n",
      "-0.011755\n",
      "-0.061718\n",
      "-0.10171\n",
      "-0.032705\n",
      "-0.045334\n",
      "-0.079288\n",
      "-0.036233\n",
      "-0.011494\n",
      "-0.093729\n",
      "-0.065215\n",
      "-0.037113\n",
      "-0.044399\n",
      "-0.084412\n",
      "0.003444\n",
      "-0.05676\n",
      "-0.07897\n",
      "-0.105367\n",
      "-0.038634\n",
      "-0.043261\n",
      "-0.139773\n",
      "-0.059094\n",
      "-0.057736\n",
      "-0.102524\n",
      "0.023881\n",
      "-0.079116\n",
      "-0.078965\n",
      "-0.161359\n",
      "-0.119376\n",
      "-0.076008\n",
      "-0.006444\n",
      "-0.026401\n"
     ]
    }
   ],
   "source": [
    "with open('acme.csv','r') as csv_file:\n",
    "    csv_reader=csv.DictReader(csv_file,delimiter=\",\")\n",
    "    line_count=0\n",
    "    for rows in csv_reader:\n",
    "        print(rows[\"market\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Python CSV reader Parameters\n",
    "\n",
    "The reader object can handle different styles of CSV files by specifying additional parameters, some of which are shown below:\n",
    "\n",
    "1.delimiter specifies the character used to separate each field. The default is the comma (',').\n",
    "\n",
    "2.quotechar specifies the character used to surround fields that contain the delimiter character. The default is a double quote (' \" ').\n",
    "\n",
    "3.escapechar specifies the character used to escape the delimiter character, in case quotes aren’t used. The default is no escape character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing CSV Files With csv\n",
    "\n",
    "You can also write to a CSV file using a writer object and the .write_row() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quotechar optional parameter tells the writer which character to use to quote fields when writing. Whether quoting is used or not, however, is determined by the quoting optional parameter:\n",
    "\n",
    "If quoting is set to csv.QUOTE_MINIMAL, then .writerow() will quote fields only if they contain the delimiter or the quotechar. This is the default case.\n",
    "\n",
    "If quoting is set to csv.QUOTE_ALL, then .writerow() will quote all fields\n",
    "\n",
    "If quoting is set to csv.QUOTE_NONNUMERIC, then .writerow() will quote all fields containing text data and convert all numeric fields to the float data type\n",
    "\n",
    "If quoting is set to csv.QUOTE_NONE, then .writerow() will escape delimiters instead of quoting them. In this case, you also must provide a value for the escapechar optional parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('employee_file.csv', 'w') as employee_file:\n",
    "    employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    employee_writer.writerow(['John Smith', 'Accounting', 'November',122])\n",
    "    employee_writer.writerow(['Erica Meyers', 'IT', 'March',1234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('acme_file.csv','w') as employee_writing:\n",
    "    employee_writer=csv.writer(employee_writing,delimiter=\",\",quotechar='\"',quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    employee_writer.writerow(['Asid Farooq','\"','Dalal Fatima','\"','Satima Dafri'])\n",
    "    employee_writer.writerow(['As Farooq','\"','Dala Fama','\"','Sa Daf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing CSV File From a Dictionary With csv\n",
    "\n",
    "Since you can read our data into a dictionary, it’s only fair that you should be able to write it out from a dictionary as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('employee_file2.csv', 'w') as csv_file:\n",
    "    fieldnames = ['emp_name', 'dept', 'birth_month']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'emp_name': 'John Smith', 'dept': 'Accounting', 'birth_month': 'November'})\n",
    "    writer.writerow({'emp_name': 'Erica Meyers', 'dept': 'IT', 'birth_month': 'March'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp_name,dept,birth_month\n",
      "\n",
      "\n",
      "\n",
      "John Smith,Accounting,November\n",
      "\n",
      "\n",
      "\n",
      "Erica Meyers,IT,March\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('employee_file2.csv','r') as csv_file:\n",
    "    for file in csv_file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike DictReader, the fieldnames parameter is required when writing a dictionary. This makes sense, when you think about it: without a list of fieldnames, the DictWriter can’t know which keys to use to retrieve values from your dictionaries. It also uses the keys in fieldnames to write out the first row as column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing CSV Files With the pandas Library\n",
    "\n",
    "Of course, the Python CSV library isn’t the only game in town. Reading CSV files is possible in pandas as well. It is highly recommended if you have a lot of data to analyze.\n",
    "\n",
    "pandas is an open-source Python library that provides high performance data analysis tools and easy to use data structures. pandas is available for all Python installations, but it is a key part of the Anaconda distribution and works extremely well in Jupyter notebooks to share data, code, analysis results, visualizations, and narrative text.\n",
    "\n",
    "Reading CSV Files With pandas\n",
    "\n",
    "To show some of the power of pandas CSV capabilities, I’ve created a slightly more complicated file to read, called acme.csv.\n",
    "\n",
    "\n",
    "Reading the CSV into a pandas DataFrame is quick and straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  month    market      acme\n",
      "0            1   1/86 -0.061134  0.030160\n",
      "1            2   2/86  0.008220 -0.165457\n",
      "2            3   3/86 -0.007381  0.080137\n",
      "3            4   4/86 -0.067561 -0.109917\n",
      "4            5   5/86 -0.006238 -0.114853\n",
      "5            6   6/86 -0.044251 -0.099254\n",
      "6            7   7/86 -0.112070 -0.226846\n",
      "7            8   8/86  0.030226  0.073445\n",
      "8            9   9/86 -0.129556 -0.143064\n",
      "9           10  10/86  0.001319  0.034776\n",
      "10          11  11/86 -0.033679 -0.063375\n",
      "11          12  12/86 -0.072795 -0.058735\n",
      "12          13   1/87  0.073396  0.050214\n",
      "13          14   2/87 -0.011618  0.111165\n",
      "14          15   3/87 -0.026852 -0.127492\n",
      "15          16   4/87 -0.040356  0.054522\n",
      "16          17   5/87 -0.047539 -0.072918\n",
      "17          18   6/87 -0.001732 -0.058979\n",
      "18          19   7/87 -0.008899  0.236147\n",
      "19          20   8/87 -0.020837 -0.094778\n",
      "20          21   9/87 -0.084811 -0.135669\n",
      "21          22  10/87 -0.262077 -0.284796\n",
      "22          23  11/87 -0.110167 -0.171494\n",
      "23          24  12/87  0.034955  0.242616\n",
      "24          25   1/88  0.012688 -0.063518\n",
      "25          26   2/88 -0.002170 -0.117677\n",
      "26          27   3/88 -0.073462  0.201674\n",
      "27          28   4/88 -0.043419 -0.147728\n",
      "28          29   5/88 -0.054730 -0.170885\n",
      "29          30   6/88 -0.011755 -0.014893\n",
      "30          31   7/88 -0.061718 -0.110515\n",
      "31          32   8/88 -0.101710 -0.168769\n",
      "32          33   9/88 -0.032705 -0.135585\n",
      "33          34  10/88 -0.045334 -0.084077\n",
      "34          35  11/88 -0.079288 -0.164550\n",
      "35          36  12/88 -0.036233  0.150269\n",
      "36          37   1/89 -0.011494 -0.015672\n",
      "37          38   2/89 -0.093729 -0.037860\n",
      "38          39   3/89 -0.065215 -0.074712\n",
      "39          40   4/89 -0.037113 -0.108530\n",
      "40          41   5/89 -0.044399 -0.036769\n",
      "41          42   6/89 -0.084412  0.023912\n",
      "42          43   7/89  0.003444 -0.078430\n",
      "43          44   8/89 -0.056760 -0.132199\n",
      "44          45   9/89 -0.078970 -0.110141\n",
      "45          46  10/89 -0.105367 -0.126302\n",
      "46          47  11/89 -0.038634 -0.095730\n",
      "47          48  12/89 -0.043261  0.065740\n",
      "48          49   1/90 -0.139773 -0.120056\n",
      "49          50   2/90 -0.059094 -0.085205\n",
      "50          51   3/90 -0.057736 -0.130433\n",
      "51          52   4/90 -0.102524 -0.116728\n",
      "52          53   5/90  0.023881 -0.078039\n",
      "53          54   6/90 -0.079116 -0.170322\n",
      "54          55   7/90 -0.078965 -0.077727\n",
      "55          56   8/90 -0.161359 -0.277035\n",
      "56          57   9/90 -0.119376 -0.207595\n",
      "57          58  10/90 -0.076008 -0.070515\n",
      "58          59  11/90 -0.006444 -0.046274\n",
      "59          60  12/90 -0.026401 -0.190834\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('acme.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it: three lines of code, and only one of them is doing the actual work. pandas.read_csv() opens, analyzes, and reads the CSV file provided, and stores the data in a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few points worth noting:\n",
    "\n",
    "First, pandas recognized that the first line of the CSV contained column names, and used them automatically. I call this Goodness.\n",
    "\n",
    "However, pandas is also using zero-based integer indices in the DataFrame. That’s because we didn’t tell it what our index should be.\n",
    "\n",
    "Further, if you look at the data types of our columns , you’ll see pandas has properly converted the Salary and Sick Days remaining columns to numbers, but the Hire Date column is still a String. This is easily confirmed in interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
